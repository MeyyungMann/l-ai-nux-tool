version: '3.8'

services:
  lai-nux-tool:
    build: 
      context: .
      args:
        - HF_HUB_DISABLE_SYMLINKS_WARNING=1
        - HF_HUB_DISABLE_TELEMETRY=1
    image: lai-nux-tool:latest
    container_name: lai-nux-tool
    runtime: nvidia  # Enable NVIDIA GPU support
    environment:
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      # Optimize HuggingFace downloads
      HF_HUB_DISABLE_SYMLINKS_WARNING: 1
      HF_HUB_DISABLE_TELEMETRY: 1
      # Optimize PyTorch
      TORCH_CUDA_ARCH_LIST: "8.9"  # RTX 5090 architecture
      PYTORCH_CUDA_ALLOC_CONF: "max_split_size_mb:512"
    volumes:
      # Mount current directory for development
      - .:/app
      # Mount home directory for configuration
      - ~/.config:/root/.config
    stdin_open: true
    tty: true
    # Increase shared memory for model loading
    shm_size: 16g
    # Set memory limits to prevent OOM
    deploy:
      resources:
        limits:
          memory: 24G
        reservations:
          memory: 8G
    command: interactive

  # Lightweight test service
  lai-nux-tool-test:
    build: 
      context: .
      args:
        - HF_HUB_DISABLE_SYMLINKS_WARNING=1
        - HF_HUB_DISABLE_TELEMETRY=1
    image: lai-nux-tool:latest
    container_name: lai-nux-tool-test
    runtime: nvidia
    environment:
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      HF_HUB_DISABLE_SYMLINKS_WARNING: 1
      HF_HUB_DISABLE_TELEMETRY: 1
    volumes:
      - .:/app
    shm_size: 8g
    command: test

  # Shell access for debugging
  lai-nux-tool-shell:
    build: 
      context: .
      args:
        - HF_HUB_DISABLE_SYMLINKS_WARNING=1
        - HF_HUB_DISABLE_TELEMETRY=1
    image: lai-nux-tool:latest
    container_name: lai-nux-tool-shell
    runtime: nvidia
    environment:
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      HF_HUB_DISABLE_SYMLINKS_WARNING: 1
      HF_HUB_DISABLE_TELEMETRY: 1
    volumes:
      - .:/app
    shm_size: 8g
    command: bash

  # RAG Mode - CodeLlama + Linux Documentation Retrieval
  # Usage: docker-compose run --rm lai-nux-tool-rag
  lai-nux-tool-rag:
    build: 
      context: .
      args:
        - HF_HUB_DISABLE_SYMLINKS_WARNING=1
        - HF_HUB_DISABLE_TELEMETRY=1
    image: lai-nux-tool:latest
    container_name: lai-nux-tool-rag
    runtime: nvidia
    environment:
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      HF_HUB_DISABLE_SYMLINKS_WARNING: 1
      HF_HUB_DISABLE_TELEMETRY: 1
      TORCH_CUDA_ARCH_LIST: "8.9"
      PYTORCH_CUDA_ALLOC_CONF: "max_split_size_mb:512"
      PYTHONUNBUFFERED: 1
      TRANSFORMERS_VERBOSITY: error  # Suppress deprecation warnings
      OPENAI_API_KEY: ${OPENAI_API_KEY}  # Pass API key from .env for online fallback
    volumes:
      # Mount RAG cache directories (shared with host)
      - ./rag_cache:/root/.lai-nux-tool/rag_cache
      - ./doc_cache:/root/.lai-nux-tool/doc_cache
      # Mount app directory
      - .:/app
      # Mount home directory for configuration
      - ~/.config:/root/.config
    stdin_open: true
    tty: true
    shm_size: 16g
    deploy:
      resources:
        limits:
          memory: 24G
        reservations:
          memory: 8G
    entrypoint: ["python", "gen_cmd.py", "--mode", "online-rag", "--interactive"]
    profiles: ["manual"]  # Don't start with 'up', use 'run' instead

  # Compare Mode - Run all modes and compare results
  lai-nux-tool-compare:
    build: 
      context: .
      args:
        - HF_HUB_DISABLE_SYMLINKS_WARNING=1
        - HF_HUB_DISABLE_TELEMETRY=1
    image: lai-nux-tool:latest
    container_name: lai-nux-tool-compare
    runtime: nvidia
    environment:
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      HF_HUB_DISABLE_SYMLINKS_WARNING: 1
      HF_HUB_DISABLE_TELEMETRY: 1
      TORCH_CUDA_ARCH_LIST: "8.9"
      PYTORCH_CUDA_ALLOC_CONF: "max_split_size_mb:512"
    volumes:
      - ./rag_cache:/root/.lai-nux-tool/rag_cache
      - ./doc_cache:/root/.lai-nux-tool/doc_cache
      - .:/app
      - ~/.config:/root/.config
    stdin_open: true
    tty: true
    shm_size: 16g
    deploy:
      resources:
        limits:
          memory: 24G
        reservations:
          memory: 8G
    command: ["python", "gen_cmd.py", "--mode", "compare", "--interactive"]

  # Ollama Mode - GPT-OSS model via Ollama
  # Usage: docker-compose run --rm lai-nux-tool-ollama
  lai-nux-tool-ollama:
    build: 
      context: .
      args:
        - HF_HUB_DISABLE_SYMLINKS_WARNING=1
        - HF_HUB_DISABLE_TELEMETRY=1
    image: lai-nux-tool:latest
    container_name: lai-nux-tool-ollama
    runtime: nvidia
    environment:
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      HF_HUB_DISABLE_SYMLINKS_WARNING: 1
      HF_HUB_DISABLE_TELEMETRY: 1
      TORCH_CUDA_ARCH_LIST: "8.9"
      PYTORCH_CUDA_ALLOC_CONF: "max_split_size_mb:512"
      PYTHONUNBUFFERED: 1
      TRANSFORMERS_VERBOSITY: error
    volumes:
      # Mount Ollama models cache
      - ./ollama_models:/root/.ollama
      # Mount app directory
      - .:/app
      # Mount home directory for configuration
      - ~/.config:/root/.config
    stdin_open: true
    tty: true
    shm_size: 16g
    deploy:
      resources:
        limits:
          memory: 24G
        reservations:
          memory: 8G
    command: ["bash", "-c", "bash /setup_ollama.sh && python gen_cmd.py --mode ollama --interactive"]
    profiles: ["manual"]

  # Ollama RAG Mode - GPT-OSS + Linux Documentation Retrieval
  # Usage: docker-compose run --rm lai-nux-tool-ollama-rag
  lai-nux-tool-ollama-rag:
    build: 
      context: .
      args:
        - HF_HUB_DISABLE_SYMLINKS_WARNING=1
        - HF_HUB_DISABLE_TELEMETRY=1
    image: lai-nux-tool:latest
    container_name: lai-nux-tool-ollama-rag
    runtime: nvidia
    environment:
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      HF_HUB_DISABLE_SYMLINKS_WARNING: 1
      HF_HUB_DISABLE_TELEMETRY: 1
      TORCH_CUDA_ARCH_LIST: "8.9"
      PYTORCH_CUDA_ALLOC_CONF: "max_split_size_mb:512"
      PYTHONUNBUFFERED: 1
      TRANSFORMERS_VERBOSITY: error
    volumes:
      # Mount Ollama models cache
      - ./ollama_models:/root/.ollama
      # Mount RAG cache directories
      - ./rag_cache:/root/.lai-nux-tool/rag_cache
      - ./doc_cache:/root/.lai-nux-tool/doc_cache
      # Mount app directory
      - .:/app
      # Mount home directory for configuration
      - ~/.config:/root/.config
    stdin_open: true
    tty: true
    shm_size: 16g
    deploy:
      resources:
        limits:
          memory: 24G
        reservations:
          memory: 8G
    command: ["bash", "-c", "bash /setup_ollama.sh && python gen_cmd.py --mode ollama-rag --interactive"]
    profiles: ["manual"]